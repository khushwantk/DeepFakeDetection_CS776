{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-15T21:20:48.681167Z",
     "iopub.status.busy": "2025-02-15T21:20:48.680805Z",
     "iopub.status.idle": "2025-02-15T21:20:49.986704Z",
     "shell.execute_reply": "2025-02-15T21:20:49.985539Z",
     "shell.execute_reply.started": "2025-02-15T21:20:48.681131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fashionmnist/t10k-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/t10k-images-idx3-ubyte\n",
      "/kaggle/input/fashionmnist/fashion-mnist_test.csv\n",
      "/kaggle/input/fashionmnist/fashion-mnist_train.csv\n",
      "/kaggle/input/fashionmnist/train-labels-idx1-ubyte\n",
      "/kaggle/input/fashionmnist/train-images-idx3-ubyte\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T12:53:43.898008Z",
     "iopub.status.busy": "2025-02-16T12:53:43.897646Z",
     "iopub.status.idle": "2025-02-16T12:53:43.904304Z",
     "shell.execute_reply": "2025-02-16T12:53:43.903490Z",
     "shell.execute_reply.started": "2025-02-16T12:53:43.897967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T12:53:49.355961Z",
     "iopub.status.busy": "2025-02-16T12:53:49.355689Z",
     "iopub.status.idle": "2025-02-16T12:53:49.377189Z",
     "shell.execute_reply": "2025-02-16T12:53:49.376254Z",
     "shell.execute_reply.started": "2025-02-16T12:53:49.355943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Copied directly from Question 1\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def gelu_derivative(x):\n",
    "    s = np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)\n",
    "    tanh_s = np.tanh(s)\n",
    "    sech2_s = 1 - tanh_s**2\n",
    "\n",
    "    return 0.5 * (1 + tanh_s) + 0.5 * x * sech2_s * np.sqrt(2 / np.pi) * (1 + 0.134145 * x**2)\n",
    "\n",
    "def softmax(x):\n",
    "    shifted = x - np.max(x, axis=0, keepdims=True)\n",
    "    exps = np.exp(shifted)\n",
    "    return exps / np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "    return loss\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred_labels == y_true_labels)\n",
    "\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations, dropout_rate=0.0):\n",
    "        self.layers = []\n",
    "        self.dropout_rate = dropout_rate\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # He initialization for ReLU, Xavier for Tanh\n",
    "            if activations[i] == 'softmax':\n",
    "                scale = 0.01  # Smaller scale for softmax layer\n",
    "            elif activations[i] in ['relu', 'leaky_relu', 'gelu']:\n",
    "                scale = np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                scale = np.sqrt(1.0 / layer_sizes[i])  # For Tanh/Sigmoid\n",
    "            W = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * scale\n",
    "            b = np.zeros((layer_sizes[i+1], 1))\n",
    "            self.layers.append({\n",
    "                'W': W,\n",
    "                'b': b,\n",
    "                'activation': activations[i]\n",
    "            })\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        A = X.T  # (input_size, batch_size)\n",
    "        cache = []\n",
    "        for layer in self.layers:\n",
    "            Z = layer['W'] @ A + layer['b']\n",
    "            activation = layer['activation']\n",
    "            if activation == 'relu':\n",
    "                A = relu(Z)\n",
    "            elif activation == 'leaky_relu':\n",
    "                A = leaky_relu(Z)\n",
    "            elif activation == 'tanh':\n",
    "                A = tanh(Z)\n",
    "            elif activation == 'gelu':\n",
    "                A = gelu(Z)\n",
    "            elif activation == 'softmax':\n",
    "                A = softmax(Z)\n",
    "            # Applying dropout (skip for output layer)\n",
    "            if training and activation != 'softmax' and self.dropout_rate > 0:\n",
    "                mask = (np.random.rand(*A.shape) < (1 - self.dropout_rate)) / (1 - self.dropout_rate)\n",
    "                A *= mask\n",
    "                cache.append({'Z': Z, 'A': A, 'mask': mask})\n",
    "            else:\n",
    "                cache.append({'Z': Z, 'A': A})\n",
    "        return A.T, cache  # (batch_size, output_size), cache\n",
    "\n",
    "    def backward(self, X, y_true, learning_rate, cache):\n",
    "        y_pred = cache[-1]['A'].T\n",
    "        m = y_true.shape[0]\n",
    "        dZ = (y_pred - y_true).T / m\n",
    "\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            layer_cache = cache[i]\n",
    "            Z = layer_cache['Z']\n",
    "            A_prev = X.T if i == 0 else cache[i-1]['A']\n",
    "\n",
    "            # gradients for weights/biases\n",
    "            dW = dZ @ A_prev.T\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "            # Update parameters\n",
    "            layer['W'] -= learning_rate * dW\n",
    "            layer['b'] -= learning_rate * db\n",
    "\n",
    "            # Propagate gradient to previous layer\n",
    "            if i > 0:\n",
    "                dA = self.layers[i]['W'].T @ dZ\n",
    "                # Apply dropout mask if present\n",
    "                if 'mask' in cache[i-1]:\n",
    "                    dA *= cache[i-1]['mask']\n",
    "                # Get Z from the PREVIOUS layer's cache\n",
    "                Z_prev = cache[i-1]['Z']\n",
    "                # Compute dZ using activation derivative of PREVIOUS layer\n",
    "                activation = self.layers[i-1]['activation']\n",
    "                if activation == 'relu':\n",
    "                    dZ = dA * relu_derivative(Z_prev)  # Use Z_prev\n",
    "                elif activation == 'leaky_relu':\n",
    "                    dZ = dA * leaky_relu_derivative(Z_prev)\n",
    "                elif activation == 'tanh':\n",
    "                    dZ = dA * tanh_derivative(Z_prev)\n",
    "                elif activation == 'gelu':\n",
    "                    dZ = dA * gelu_derivative(Z_prev)\n",
    "\n",
    "\n",
    "def train_mlp(x_train, y_train, x_val, y_val, layer_sizes, activations,\n",
    "              epochs=10, batch_size=64, lr=0.01, dropout_rate=0.0,\n",
    "              patience=5, delta=0.001):\n",
    "\n",
    "    print(layer_sizes,activations)\n",
    "\n",
    "    mlp = MLP(layer_sizes, activations, dropout_rate)\n",
    "\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_epoch = 0\n",
    "    wait = 0  # Counter for epochs without improvement for early stopping\n",
    "    best_model = None  # Store the best model state\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(x_train.shape[0])\n",
    "        x_shuffled = x_train[permutation]\n",
    "        y_shuffled = y_train[permutation]\n",
    "\n",
    "        epoch_train_loss, epoch_train_acc = 0, 0\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            X_batch = x_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            y_pred, cache = mlp.forward(X_batch, training=True)\n",
    "            batch_loss = cross_entropy_loss(y_pred, y_batch)\n",
    "            batch_acc = accuracy(y_pred, y_batch)\n",
    "            epoch_train_loss += batch_loss * len(X_batch)\n",
    "            epoch_train_acc += batch_acc * len(X_batch)\n",
    "            mlp.backward(X_batch, y_batch, lr, cache)\n",
    "\n",
    "        # Compute training metrics\n",
    "        epoch_train_loss /= len(x_train)\n",
    "        epoch_train_acc /= len(x_train)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        # Compute validation metrics\n",
    "        val_pred, _ = mlp.forward(x_val, training=False)\n",
    "        val_loss = cross_entropy_loss(val_pred, y_val)\n",
    "        val_acc = accuracy(val_pred, y_val)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss - delta:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}\")\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            wait = 0\n",
    "            # Save the best model weights\n",
    "            best_model = [\n",
    "                {'W': np.copy(layer['W']), 'b': np.copy(layer['b'])}\n",
    "                for layer in mlp.layers\n",
    "            ]\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1} (best epoch: {best_epoch+1})\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"Patience: {wait}/{patience}\")\n",
    "\n",
    "    # Restore the best model weights\n",
    "    if best_model is not None:\n",
    "        for i, layer in enumerate(mlp.layers):\n",
    "            layer['W'] = best_model[i]['W']\n",
    "            layer['b'] = best_model[i]['b']\n",
    "\n",
    "    return mlp, train_losses, train_accs, val_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:23:42.431604Z",
     "iopub.status.busy": "2025-02-16T14:23:42.431286Z",
     "iopub.status.idle": "2025-02-16T14:26:01.676747Z",
     "shell.execute_reply": "2025-02-16T14:26:01.675883Z",
     "shell.execute_reply.started": "2025-02-16T14:23:42.431576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "Initializing Feature Extractor Training...\n",
      "max xavier [32, 64, 128, 256, 512] MLP Hidden Layer:  1024\n",
      "512\n",
      "Epoch 1/10 | Train Loss: 0.6960 | Train Acc: 0.7332 | Val Loss: 0.4207 | Val Acc: 0.8477\n",
      "Epoch 2/10 | Train Loss: 0.3632 | Train Acc: 0.8696 | Val Loss: 0.3247 | Val Acc: 0.8832\n",
      "Epoch 3/10 | Train Loss: 0.2888 | Train Acc: 0.8961 | Val Loss: 0.2785 | Val Acc: 0.8988\n",
      "Epoch 4/10 | Train Loss: 0.2472 | Train Acc: 0.9104 | Val Loss: 0.2575 | Val Acc: 0.9087\n",
      "Epoch 5/10 | Train Loss: 0.2152 | Train Acc: 0.9217 | Val Loss: 0.2663 | Val Acc: 0.9073\n",
      "Epoch 6/10 | Train Loss: 0.1903 | Train Acc: 0.9305 | Val Loss: 0.2569 | Val Acc: 0.9090\n",
      "Epoch 7/10 | Train Loss: 0.1673 | Train Acc: 0.9390 | Val Loss: 0.2778 | Val Acc: 0.9130\n",
      "Epoch 8/10 | Train Loss: 0.1467 | Train Acc: 0.9460 | Val Loss: 0.2702 | Val Acc: 0.9141\n",
      "Epoch 9/10 | Train Loss: 0.1278 | Train Acc: 0.9527 | Val Loss: 0.3052 | Val Acc: 0.9042\n",
      "Epoch 10/10 | Train Loss: 0.1105 | Train Acc: 0.9593 | Val Loss: 0.2915 | Val Acc: 0.9157\n",
      "Model Training Completed\n",
      "Extracted feature shapes: (48000, 512) (12000, 512) (10000, 512)\n",
      "\n",
      "Test Accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Data Loading and Preprocessing\n",
    "# ------------------------------\n",
    "train_df = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')\n",
    "test_df  = pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_test.csv')\n",
    "\n",
    "# Preprocessing with correct normalization\n",
    "train_data = np.array(train_df, dtype='float32')\n",
    "test_data  = np.array(test_df, dtype='float32')\n",
    "X_train = (train_data[:, 1:] / 255.0 - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "y_train = train_data[:, 0].astype(np.int64)\n",
    "X_test  = (test_data[:, 1:] / 255.0 - 0.5) / 0.5\n",
    "y_test  = test_data[:, 0].astype(np.int64)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# Define the CNN Extractor in PyTorch\n",
    "# ------------------------------\n",
    "class CNNExtractor(nn.Module):\n",
    "    def __init__(self, pool_method='max', weight_init='xavier',conv_dims=[32, 64, 128, 256, 512],n6=32):\n",
    "        super(CNNExtractor, self).__init__()\n",
    "        self.pool_method = pool_method\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.n6=n6\n",
    "\n",
    "        # Convolutional layers with correct filter sizes\n",
    "        self.conv1 = nn.Conv2d(1, conv_dims[0], kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_dims[0], conv_dims[1], kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(conv_dims[1], conv_dims[2], kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(conv_dims[2], conv_dims[3], kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(conv_dims[3], conv_dims[4], kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = self._get_pooling(pool_method)\n",
    "\n",
    "        # Weight initialization\n",
    "        self._initialize_weights(weight_init)\n",
    "\n",
    "\n",
    "        # Final flattened size calculation\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "            output = self.forward_features(dummy_input)\n",
    "        self.flattened_size= output.view(-1).shape[0]\n",
    "\n",
    "        print(self.flattened_size)\n",
    "\n",
    "        # FCC\n",
    "        # self.fc = nn.Linear(self.flattened_size, 10)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, self.n6),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.n6, 10)\n",
    "        )\n",
    "\n",
    "    def _get_pooling(self, method):\n",
    "        pool_dict = {\n",
    "            'max': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            'avg': nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            'global': nn.AdaptiveAvgPool2d((1, 1))\n",
    "        }\n",
    "        return pool_dict.get(method)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv5(x))\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.forward_features(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        with torch.no_grad():\n",
    "            x = self.forward_features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, method):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if method == 'xavier':\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                elif method == 'he':\n",
    "                    nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                elif method == 'random':\n",
    "                    nn.init.normal_(layer.weight, mean=0, std=0.1)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "# ------------------------------\n",
    "# Train Function with Device Handling\n",
    "# ------------------------------\n",
    "def train_cnn_extractor(epochs=10, batch_size=64, lr=0.001, pool_method='max',\n",
    "                       weight_init='he', conv_dims=[32, 64, 128, 256, 512],n6=32):\n",
    "    print(pool_method,weight_init,conv_dims,\"MLP Hidden Layer: \",n6)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert data to tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    # Create torch datasets and loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = torch.utils.data.TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model\n",
    "    model = CNNExtractor(pool_method, weight_init,conv_dims,n6=32).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            train_correct += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            train_total += inputs.size(0)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += loss_fn(outputs, targets).item() * inputs.size(0)\n",
    "                val_correct += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "                val_total += inputs.size(0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model.to('cpu')  # Return to CPU for feature extraction\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Train the Model ............Change Parameters Here\n",
    "# ------------------------------\n",
    "print(\"Initializing Feature Extractor Training...\")\n",
    "cnn_model = train_cnn_extractor(epochs=10, pool_method='max', weight_init='xavier',conv_dims=[32, 64, 128, 256,512],n6=1024,)\n",
    "print(\"Model Training Completed\")\n",
    "\n",
    "# ------------------------------\n",
    "# Feature Extraction\n",
    "# ------------------------------\n",
    "\n",
    "cnn_model.eval()\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    features_train = cnn_model.extract_features(x_train_tensor).numpy()\n",
    "    features_val = cnn_model.extract_features(x_val_tensor).numpy()\n",
    "    features_test = cnn_model.extract_features(X_test_tensor).numpy()\n",
    "\n",
    "print(\"Extracted feature shapes:\", features_train.shape, features_val.shape, features_test.shape)\n",
    "\n",
    "# Evaluate the Test Features\n",
    "# Convert numpy features to PyTorch tensors\n",
    "features_test_tensor = torch.tensor(features_test, dtype=torch.float32)\n",
    "\n",
    "# Make predictions using the model's classifier (fc layer)\n",
    "with torch.no_grad():\n",
    "    test_outputs = cnn_model.fc(features_test_tensor)\n",
    "    test_preds = torch.argmax(test_outputs, dim=1).numpy()\n",
    "\n",
    "# print(test_preds.shape)\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (test_preds == y_test).mean()\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:28:37.802794Z",
     "iopub.status.busy": "2025-02-16T14:28:37.802492Z",
     "iopub.status.idle": "2025-02-16T14:28:37.808052Z",
     "shell.execute_reply": "2025-02-16T14:28:37.807452Z",
     "shell.execute_reply.started": "2025-02-16T14:28:37.802771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "num_classes=10\n",
    "y_train_oh = to_categorical(y_train, num_classes)\n",
    "y_val_oh = to_categorical(y_val, num_classes)\n",
    "y_test_oh = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T14:28:52.228041Z",
     "iopub.status.busy": "2025-02-16T14:28:52.227759Z",
     "iopub.status.idle": "2025-02-16T14:29:19.549317Z",
     "shell.execute_reply": "2025-02-16T14:29:19.548469Z",
     "shell.execute_reply.started": "2025-02-16T14:28:52.228019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 512) (48000, 10)\n",
      "(12000, 512) (12000, 10)\n",
      "(10000, 512) (10000, 10)\n",
      "Training custom MLP on CNN features...\n",
      "[512, 1024, 10] ['relu', 'softmax']\n",
      "Validation loss improved from inf to 0.2853\n",
      "Epoch 1/200 | Train Loss: 0.1045 | Train Acc: 0.9697 | Val Loss: 0.2853 | Val Acc: 0.9192 | Patience: 0/5\n",
      "Epoch 2/200 | Train Loss: 0.0796 | Train Acc: 0.9728 | Val Loss: 0.2960 | Val Acc: 0.9184 | Patience: 1/5\n",
      "Epoch 3/200 | Train Loss: 0.0757 | Train Acc: 0.9740 | Val Loss: 0.2954 | Val Acc: 0.9204 | Patience: 2/5\n",
      "Epoch 4/200 | Train Loss: 0.0749 | Train Acc: 0.9739 | Val Loss: 0.2938 | Val Acc: 0.9197 | Patience: 3/5\n",
      "Epoch 5/200 | Train Loss: 0.0726 | Train Acc: 0.9740 | Val Loss: 0.2972 | Val Acc: 0.9197 | Patience: 4/5\n",
      "\n",
      "Early stopping at epoch 6 (best epoch: 1)\n",
      "Custom MLP training complete.\n",
      "Test Accuracy (Custom MLP on CNN features): 0.9235\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape,y_train_oh.shape)\n",
    "print(features_val.shape,y_val_oh.shape)\n",
    "print(features_test.shape,y_test_oh.shape)\n",
    "# ------------------------------\n",
    "# Our Custom MLP (NumPy-based) on the extracted features\n",
    "# ------------------------------\n",
    "layer_sizes = [features_train.shape[1], 1024,10]\n",
    "# Note: If you originally used more layers, adjust accordingly.\n",
    "activations = ['relu', 'softmax']\n",
    "\n",
    "print(\"Training custom MLP on CNN features...\")\n",
    "mlp_model, train_losses, train_accs, val_losses, val_accs = train_mlp(\n",
    "    features_train, y_train_oh,\n",
    "    features_val, y_val_oh,\n",
    "    layer_sizes, activations,\n",
    "    epochs=200, lr=0.001, dropout_rate=0.2, patience=5, batch_size=32\n",
    ")\n",
    "print(\"Custom MLP training complete.\")\n",
    "\n",
    "\n",
    "# Use your custom MLP for prediction.\n",
    "test_pred, _ = mlp_model.forward(features_test, training=False)\n",
    "test_acc = accuracy(test_pred, y_test_oh)\n",
    "print(f\"Test Accuracy (Custom MLP on CNN features): {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot loss curves\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(train_losses, label='Training Loss')\n",
    "# plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Loss vs. Epochs')\n",
    "\n",
    "# # Plot accuracy curves\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(train_accs, label='Training Accuracy')\n",
    "# plt.plot(val_accs, label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('Accuracy vs. Epochs')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2243,
     "sourceId": 9243,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
